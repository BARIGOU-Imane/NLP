# -*- coding: utf-8 -*-
"""tp1_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wNevmWFVwYp_aQMTnJWr57fAmr34ufM-
"""

!pip install kaggle
from google.colab import files
files.upload()  # Vous permet de téléverser le fichier kaggle.json depuis votre ordinateur vers Colab.
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d thoughtvector/customer-support-on-twitter

!unzip customer-support-on-twitter.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('/content/sample.csv')
df

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Chargement de l'ensemble de données
df = pd.read_csv('/content/sample.csv')

# Supprimer les doublons
df.drop_duplicates(inplace=True)

# Nettoyage du texte
def clean_text(text):
    # Supprimer la ponctuation, les chiffres et les caractères spéciaux
    text = text.replace(r'[^\w\s]', '')
    # Mettre en minuscules
    text = text.lower()
    # Supprimer les mentions d'utilisateurs et les liens
    text = ' '.join(word for word in text.split() if not word.startswith('@') and not word.startswith('http'))
    return text

df['text'] = df['text'].apply(clean_text)


# Supprimer les mots vides (stop words)
#nltk.download('stopwords')
#stop_words = set(stopwords.words('english'))

# Lemmatisation
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

# Enregistrez le jeu de données nettoyé au format CSV
df.to_csv('donnees_nettoyees.csv', index=False)
df